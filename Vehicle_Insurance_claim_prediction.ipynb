{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the given csv file\n",
    "\n",
    "df = pd.read_csv('./data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values are either marked as \" \" or '?' but for ease of handling fill NaN with \"?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " df = df.fillna('?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Pre processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Method to deal with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row_ID : 0\n",
      "Household_ID : 0\n",
      "Vehicle : 0\n",
      "Calendar_Year : 0\n",
      "Model_Year : 0\n",
      "Blind_Make : 15\n",
      "Blind_Model : 15\n",
      "Blind_Submodel : 15\n",
      "Cat1 : 50\n",
      "Cat2 : 10591\n",
      "Cat3 : 11\n",
      "Cat4 : 12985\n",
      "Cat5 : 12998\n",
      "Cat6 : 50\n",
      "Cat7 : 16480\n",
      "Cat8 : 2\n",
      "Cat9 : 0\n",
      "Cat10 : 10\n",
      "Cat11 : 58\n",
      "Cat12 : 52\n",
      "OrdCat : 19\n",
      "Var1 : 0\n",
      "Var2 : 0\n",
      "Var3 : 0\n",
      "Var4 : 0\n",
      "Var5 : 0\n",
      "Var6 : 0\n",
      "Var7 : 0\n",
      "Var8 : 0\n",
      "NVCat : 0\n",
      "NVVar1 : 0\n",
      "NVVar2 : 0\n",
      "NVVar3 : 0\n",
      "NVVar4 : 0\n",
      "Claim_Amount : 0\n"
     ]
    }
   ],
   "source": [
    "# First let's see how much missing data each column has\n",
    "\n",
    "for i in df.columns:\n",
    "    print(i + ' : ' + str(df[i].isin(['?']).sum().astype(int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We have many methods to handle with missing data like Case Deletion, Mean Substitution, Regression Imputation ** etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3668100/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choosing the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Row_ID', 'Household_ID', 'Vehicle', 'Calendar_Year', 'Model_Year',\n",
       "       'Blind_Make', 'Blind_Model', 'Blind_Submodel', 'Cat1', 'Cat3', 'Cat6',\n",
       "       'Cat8', 'Cat9', 'Cat10', 'Cat11', 'Cat12', 'OrdCat', 'Var1', 'Var2',\n",
       "       'Var3', 'Var4', 'Var5', 'Var6', 'Var7', 'Var8', 'NVCat', 'NVVar1',\n",
       "       'NVVar2', 'NVVar3', 'NVVar4', 'Claim_Amount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here I have choosen Cat 1, Cat 3, Cat 6, Cat 8, Cat 9, Cat 10, Cat11, Cat12\n",
    "# As we can see from the code above that they have the least number of missing data \n",
    "\n",
    "df = df.drop(columns = ['Cat2', 'Cat4', 'Cat5', 'Cat7'])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now that I have dropped features with so much missing data we can see below the total rows with missing data in any of the features constitutes of only 0.3 % of the complete case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row_ID : 0\n",
      "Household_ID : 0\n",
      "Vehicle : 0\n",
      "Calendar_Year : 0\n",
      "Model_Year : 0\n",
      "Blind_Make : 15\n",
      "Blind_Model : 15\n",
      "Blind_Submodel : 15\n",
      "Cat1 : 50\n",
      "Cat3 : 11\n",
      "Cat6 : 50\n",
      "Cat8 : 2\n",
      "Cat9 : 0\n",
      "Cat10 : 10\n",
      "Cat11 : 58\n",
      "Cat12 : 52\n",
      "OrdCat : 19\n",
      "Var1 : 0\n",
      "Var2 : 0\n",
      "Var3 : 0\n",
      "Var4 : 0\n",
      "Var5 : 0\n",
      "Var6 : 0\n",
      "Var7 : 0\n",
      "Var8 : 0\n",
      "NVCat : 0\n",
      "NVVar1 : 0\n",
      "NVVar2 : 0\n",
      "NVVar3 : 0\n",
      "NVVar4 : 0\n",
      "Claim_Amount : 0\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(i + ' : ' + str(df[i].isin(['?']).sum().astype(int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### So I have chosen Listwise or Case Deletion in this particular case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Cat1'] != '?']\n",
    "df = df[df['Cat3'] != '?']\n",
    "df = df[df['Cat6'] != '?']\n",
    "df = df[df['Cat8'] != '?']\n",
    "df = df[df['Cat9'] != '?']\n",
    "df = df[df['Cat10'] != '?']\n",
    "df = df[df['Cat11'] != '?']\n",
    "df = df[df['Cat12'] != '?']\n",
    "df = df[df['Blind_Make'] != '?']\n",
    "df = df[df['Blind_Model'] != '?']\n",
    "df = df[df['Blind_Submodel'] != '?']\n",
    "df = df[df['OrdCat'] != '?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row_ID : 0\n",
      "Household_ID : 0\n",
      "Vehicle : 0\n",
      "Calendar_Year : 0\n",
      "Model_Year : 0\n",
      "Blind_Make : 0\n",
      "Blind_Model : 0\n",
      "Blind_Submodel : 0\n",
      "Cat1 : 0\n",
      "Cat3 : 0\n",
      "Cat6 : 0\n",
      "Cat8 : 0\n",
      "Cat9 : 0\n",
      "Cat10 : 0\n",
      "Cat11 : 0\n",
      "Cat12 : 0\n",
      "OrdCat : 0\n",
      "Var1 : 0\n",
      "Var2 : 0\n",
      "Var3 : 0\n",
      "Var4 : 0\n",
      "Var5 : 0\n",
      "Var6 : 0\n",
      "Var7 : 0\n",
      "Var8 : 0\n",
      "NVCat : 0\n",
      "NVVar1 : 0\n",
      "NVVar2 : 0\n",
      "NVVar3 : 0\n",
      "NVVar4 : 0\n",
      "Claim_Amount : 0\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(i + ' : ' + str(df[i].isin(['?']).sum().astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update df to new values without missing values\n",
    "\n",
    "df.to_csv('./data/train-copy.csv', index=False)\n",
    "df = pd.read_csv('./data/train-copy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({'Row_ID': 'float64', 'Household_ID': 'float64', 'Vehicle': 'float64', 'Calendar_Year': 'float64', 'Model_Year': 'float64', 'OrdCat': 'float64',})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Convert categorical values to a suitable representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here I have used one-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_cat = ['Cat1', 'Cat3', 'Cat6', 'Cat8', 'Cat9', 'Cat10', 'Cat11', 'Cat12', 'OrdCat', 'NVCat']\n",
    "attributes_num = ['Household_ID', 'Vehicle', 'Calendar_Year', 'Model_Year', 'Var1', \\\n",
    "                  'Var2', 'Var3', 'Var4', 'Var5', 'Var6', 'Var7', 'Var8',\n",
    "                 'NVVar1', 'NVVar2', 'NVVar3', 'NVVar4']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply column transformation as per data type of the columns\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "full_transform = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), attributes_num),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown = \"ignore\"), attributes_cat),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Balancing the Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zeroes = df[df['Claim_Amount'] == 0]\n",
    "df_non_zeroes = df[df['Claim_Amount'] != 0]\n",
    "\n",
    "nz = df_non_zeroes['Claim_Amount'].count()\n",
    "\n",
    "z = df_zeroes['Claim_Amount'].count()\n",
    "\n",
    "if nz != z:\n",
    "    if nz > z:\n",
    "        df_non_zeroes = df_non_zeroes.sample(n = z)\n",
    "    elif nz < z:\n",
    "        df_zeroes = df_zeroes.sample(n = nz)\n",
    "else:\n",
    "    print('Data set is balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8958, 31)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zeroes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8958, 31)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_non_zeroes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge balanced dataframes to create a data set\n",
    "\n",
    "claimed_amount_data = pd.concat([df_non_zeroes,df_zeroes], ignore_index=True)\n",
    "\n",
    "claimed_amount_data = claimed_amount_data.drop('Row_ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data set into training data and testing data\n",
    "\n",
    "ca_train_set, ca_test_set = train_test_split(claimed_amount_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare attributes and labels for the data\n",
    "\n",
    "ca_train_set_attributes = ca_train_set.drop('Claim_Amount', axis=1)\n",
    "ca_train_set_labels = ca_train_set['Claim_Amount']\n",
    "\n",
    "ca_train_set_attributes_prepared = full_transform.fit_transform(ca_train_set_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Performance using a single model\n",
    "#### a. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(ca_train_set_attributes_prepared, ca_train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and prepare the data set further into training and validation sets\n",
    "\n",
    "ca_train2_set, ca_val_set = train_test_split(ca_train_set, test_size=0.1, random_state=42)\n",
    "ca_train2_set_attributes = ca_train2_set.drop('Claim_Amount', axis=1)\n",
    "ca_train2_set_labels = ca_train2_set['Claim_Amount']\n",
    "ca_val_set_attributes = ca_val_set.drop('Claim_Amount', axis=1)\n",
    "ca_val_set_labels = ca_val_set['Claim_Amount']\n",
    "\n",
    "ca_train2_set_all_attributes = full_transform.fit_transform(ca_train2_set_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Linear Regression model upon the training data\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(ca_train2_set_all_attributes, ca_train2_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_val_set_all_attributes = full_transform.transform(ca_val_set_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326.9383410126069"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the performance of the model by calculating performance metrics\n",
    "\n",
    "ca_val_set_predictions = lin_reg.predict(ca_val_set_all_attributes)\n",
    "error = np.sqrt(mean_squared_error(ca_val_set_labels, ca_val_set_predictions))\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid Search to fine tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_values = {'copy_X': [True, False], 'fit_intercept': [True, False], 'normalize': [True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lin_reg = GridSearchCV(lin_reg, param_grid = grid_values,scoring = 'neg_root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LinearRegression(),\n",
       "             param_grid={'copy_X': [True, False],\n",
       "                         'fit_intercept': [True, False],\n",
       "                         'normalize': [True, False]},\n",
       "             scoring='neg_root_mean_squared_error')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the tuned model on the same data set as before\n",
    "\n",
    "grid_lin_reg.fit(ca_train2_set_all_attributes, ca_train2_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326.9600882041836"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the performance of the tuned model by calculating performance metrics\n",
    "\n",
    "ca_val_set_predictions = grid_lin_reg.predict(ca_val_set_all_attributes)\n",
    "\n",
    "error = np.sqrt(mean_squared_error(ca_val_set_labels, ca_val_set_predictions))\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE has slightly improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://github.com/maalvarezl/MLAI-Labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Ridge regression model on the training data\n",
    "\n",
    "rr.fit(ca_train2_set_all_attributes, ca_train2_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_rr= rr.predict(ca_train2_set_all_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://www.pluralsight.com/guides/linear-lasso-ridge-regression-scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304.4978484304715"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the performance of the tuned model by calculating performance metrics\n",
    "\n",
    "error = np.sqrt(mean_squared_error(ca_train2_set_labels, pred_train_rr))\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid Search to fine tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['alpha', 'copy_X', 'fit_intercept', 'max_iter', 'normalize', 'random_state', 'solver', 'tol'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rr = {'alpha': [0.01,0.1,1], 'copy_X': [True, False], 'fit_intercept': [True, False],\n",
    "                 'normalize': [True, False], 'random_state': [0,42], \n",
    "                 'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}\n",
    "grid_rr = GridSearchCV(Ridge(), param_grid=param_grid_rr, scoring='neg_root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Ridge(),\n",
       "             param_grid={'alpha': [0.01, 0.1, 1], 'copy_X': [True, False],\n",
       "                         'fit_intercept': [True, False],\n",
       "                         'normalize': [True, False], 'random_state': [0, 42],\n",
       "                         'solver': ['auto', 'svd', 'cholesky', 'lsqr',\n",
       "                                    'sparse_cg', 'sag', 'saga']},\n",
       "             scoring='neg_root_mean_squared_error')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rr.fit(ca_train2_set_all_attributes, ca_train2_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_mod = Ridge(alpha=grid_rr.best_params_['alpha'],\n",
    "              copy_X=grid_rr.best_params_['copy_X'],\n",
    "              fit_intercept=grid_rr.best_params_['fit_intercept'],\n",
    "              normalize=grid_rr.best_params_['normalize'],\n",
    "              random_state=grid_rr.best_params_['random_state'],\n",
    "              solver=grid_rr.best_params_['solver'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1, normalize=True, random_state=0, solver='lsqr')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the tuned model on the same dataset\n",
    "\n",
    "rr_mod.fit(ca_train2_set_all_attributes, ca_train2_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_rr_mod = rr_mod.predict(ca_train2_set_all_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE on the validation data is : 304.77584434780584\n"
     ]
    }
   ],
   "source": [
    "## Computing the RMSE for the validation dataset\n",
    "error_mod_rr = np.sqrt(mean_squared_error(ca_train2_set_labels, pred_train_rr_mod))\n",
    "print('The RMSE on the validation data is :',error_mod_rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bootstrap', 'ccp_alpha', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_train2_set_attributes_Question2c = full_transform.fit_transform(ca_train2_set_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_val_set_attributes_Question2c = full_transform.transform(ca_val_set_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_train_set_attributes_Question2c = np.vstack((ca_train2_set_attributes_Question2c , ca_val_set_attributes_Question2c))\n",
    "whole_train_set_labels_Question2c = np.hstack((ca_train2_set_labels, ca_val_set_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid Search to fine tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=RandomForestRegressor(),\n",
       "             param_grid={'max_samples': [500, 1000, 2000, 3000],\n",
       "                         'n_estimators': [20, 50, 100, 200]},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the tuned Random Forest regressor on the training data\n",
    "\n",
    "n_estimators_Question2c = [20, 50, 100, 200]\n",
    "max_samples_Question2c = [500, 1000, 2000, 3000]\n",
    "param_grid_Question2c = dict(n_estimators = n_estimators_Question2c, max_samples = max_samples_Question2c)\n",
    "grid_regression_Question2c = GridSearchCV(RandomForestRegressor(), param_grid=param_grid_Question2c, scoring='neg_mean_squared_error')\n",
    "grid_regression_Question2c.fit(whole_train_set_attributes_Question2c, whole_train_set_labels_Question2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE on the validation data is : 331.50269850701153\n"
     ]
    }
   ],
   "source": [
    "regr_Question2c = RandomForestRegressor(n_estimators=grid_regression_Question2c.best_params_[\"n_estimators\"],max_samples=grid_regression_Question2c.best_params_[\"max_samples\"])\n",
    "regr_Question2c.fit(ca_train2_set_attributes_Question2c, ca_train2_set_labels)\n",
    "ca_val_set_predictions_Question2c = regr_Question2c.predict(ca_val_set_attributes_Question2c)\n",
    "\n",
    "\n",
    "## Computing the RMSE for the validation dataset\n",
    "error_mod_Question2c = np.sqrt(mean_squared_error(ca_val_set_labels, ca_val_set_predictions_Question2c))\n",
    "print('The RMSE on the validation data is :',error_mod_Question2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['alpha', 'ccp_alpha', 'criterion', 'init', 'learning_rate', 'loss', 'max_depth', 'max_features', 'max_leaf_nodes', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_iter_no_change', 'presort', 'random_state', 'subsample', 'tol', 'validation_fraction', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE on the validation data is : 326.9612230549615\n"
     ]
    }
   ],
   "source": [
    "## Train the Gradient Boosting Regressor on the train set\n",
    "ca_train2_set_attributes_Question2d = full_transform.fit_transform(ca_train2_set_attributes)\n",
    "## transform in the validation set\n",
    "ca_val_set_attributes_Question2d = full_transform.transform(ca_val_set_attributes)\n",
    "\n",
    "## Concatenating the attributes and the labels\n",
    "whole_train_set_attributes_Question2d = np.vstack((ca_train2_set_attributes_Question2d , ca_val_set_attributes_Question2d))\n",
    "whole_train_set_labels_Question2d = np.hstack((ca_train2_set_labels, ca_val_set_labels))\n",
    "\n",
    "## Applying the Gradient Boosting for regression and exploring different maximum depth options\n",
    "# For simplicity, we set those values for the parameters n_estimators.\n",
    "n_estimators_Question2d = [20, 50, 100, 200]\n",
    "param_grid_Question2d = dict(n_estimators = n_estimators_Question2d)\n",
    "grid_regression_Question2d = GridSearchCV(GradientBoostingRegressor(), param_grid=param_grid_Question2d, scoring='neg_mean_squared_error')\n",
    "grid_regression_Question2d.fit(whole_train_set_attributes_Question2d, whole_train_set_labels_Question2d)\n",
    "\n",
    "## Training a Gradient Boosting using the best value for the n_estimators\n",
    "regr_Question2d = GradientBoostingRegressor(n_estimators=grid_regression_Question2d.best_params_[\"n_estimators\"])\n",
    "regr_Question2d.fit(ca_train2_set_attributes_Question2d, ca_train2_set_labels)\n",
    "ca_val_set_predictions_Question2d = regr_Question2d.predict(ca_val_set_attributes_Question2d)\n",
    "\n",
    "## Computing the RMSE for the validation dataset\n",
    "error_mod_Question2d = np.sqrt(mean_squared_error(ca_val_set_labels, ca_val_set_predictions_Question2d))\n",
    "print('The RMSE on the validation data is :',error_mod_Question2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Performance using a combination of 2 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17916, 30)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claimed_amount_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data for binary classifier\n",
    "\n",
    "claimed_amount_data_class = claimed_amount_data.copy()\n",
    "\n",
    "claimed_amount_data_class['Claim_Amount'] = (claimed_amount_data_class['Claim_Amount'] != 0).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing datasets\n",
    "\n",
    "ca_train_set_clf, ca_test_set_clf = train_test_split(claimed_amount_data_class, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the attributes and labels of the training dataset\n",
    "\n",
    "ca_train_set_clf_attributes = ca_train_set_clf.drop('Claim_Amount', axis=1)\n",
    "ca_train_set_clf_labels = ca_train_set_clf['Claim_Amount']\n",
    "\n",
    "ca_train_set_clf_attributes_prepared = full_transform.fit_transform(ca_train_set_clf_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the attributes and labels of the testing dataset\n",
    "\n",
    "ca_test_set_clf_attributes = ca_test_set_clf.drop('Claim_Amount', axis=1)\n",
    "ca_test_set_clf_labels = ca_test_set_clf['Claim_Amount']\n",
    "\n",
    "ca_test_set_clf_attributes_prepared = full_transform.transform(ca_test_set_clf_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data further into training and validation sets and prepare the data\n",
    "\n",
    "ca_train2_set_clf, ca_val_set_clf = train_test_split(ca_train_set_clf, test_size=0.15, random_state=42)\n",
    "ca_train2_set_clf_attributes = ca_train2_set_clf.drop('Claim_Amount', axis=1)\n",
    "ca_train2_set_clf_labels = ca_train2_set_clf['Claim_Amount']\n",
    "ca_val_set_clf_attributes = ca_val_set_clf.drop('Claim_Amount', axis=1)\n",
    "ca_val_set_clf_labels = ca_val_set_clf['Claim_Amount']\n",
    "\n",
    "ca_train2_set_clf_all_attributes = full_transform.fit_transform(ca_train2_set_clf_attributes)\n",
    "\n",
    "ca_val_set_clf_all_attributes = full_transform.transform(ca_val_set_clf_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Random Forest Classifier on the training data\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(ca_train2_set_clf_all_attributes, ca_train2_set_clf_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.946975614900749\n",
      "0.5382932166301969\n",
      "0.5283862315601252\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the model\n",
    "\n",
    "ca_val_set_clf_predictions = clf.predict(ca_val_set_clf_all_attributes)\n",
    "error = log_loss(ca_val_set_clf_labels, ca_val_set_clf_predictions)\n",
    "acc = accuracy_score(ca_val_set_clf_labels, ca_val_set_clf_predictions)\n",
    "f1 = f1_score(ca_val_set_clf_labels, ca_val_set_clf_predictions)\n",
    "print(error)\n",
    "print(acc)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Gradient Boosting Classifier on the data set\n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(ca_train2_set_clf_all_attributes, ca_train2_set_clf_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.372575664043442\n",
      "0.5549234135667396\n",
      "0.5375170532060027\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the classifier\n",
    "\n",
    "ca_val_set_clf_predictions = clf.predict(ca_val_set_clf_all_attributes)\n",
    "error = log_loss(ca_val_set_clf_labels, ca_val_set_clf_predictions)\n",
    "acc = accuracy_score(ca_val_set_clf_labels, ca_val_set_clf_predictions)\n",
    "f1 = f1_score(ca_val_set_clf_labels, ca_val_set_clf_predictions)\n",
    "print(error)\n",
    "print(acc)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting classifier gave better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset for Regressor where the predicted 'Claim_Amount' is non zero \n",
    "\n",
    "claimed_amount_all_attributes = claimed_amount_data.drop('Claim_Amount', axis = 1)\n",
    "\n",
    "claimed_amount_data_attributes_transformed = full_transform.fit_transform(claimed_amount_all_attributes)\n",
    "\n",
    "claimed_amount_all_labels = claimed_amount_data['Claim_Amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "claimed_amount_data_nz = claimed_amount_data_attributes_transformed[claimed_amount_data['Claim_Amount'] != 0]\n",
    "claimed_amount_labels_nz = claimed_amount_all_labels[claimed_amount_data['Claim_Amount'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test data\n",
    "\n",
    "ca_train_set_regr, ca_test_set_regr, ca_train_set_regr_labels, ca_test_set_regr_labels = train_test_split(claimed_amount_data_nz, claimed_amount_labels_nz, test_size=0.15, random_state=42)\n",
    "\n",
    "# Split the training data further into training and validation data\n",
    "\n",
    "ca_train2_set_regr, ca_val_set_regr, ca_train2_set_regr_labels, ca_val_set_regr_labels = train_test_split(ca_train_set_regr, ca_train_set_regr_labels, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1344, 79)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_test_set_regr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the linear regressor model on training data\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regr = LinearRegression()\n",
    "regr.fit(ca_train2_set_regr, ca_train2_set_regr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394.06645063665206"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the performance on validation data\n",
    "\n",
    "ca_val_set_regr_predictions = regr.predict(ca_val_set_regr)\n",
    "error = np.sqrt(mean_squared_error(ca_val_set_regr_labels, ca_val_set_regr_predictions))\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge()"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Ridge regressor model on training data\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "regr = Ridge()\n",
    "regr.fit(ca_train2_set_regr, ca_train2_set_regr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1143,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_val_set_regr_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393.96644525789264"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the performance on validation data\n",
    "\n",
    "ca_val_set_regr_predictions = regr.predict(ca_val_set_regr)\n",
    "error = np.sqrt(mean_squared_error(ca_val_set_regr_labels, ca_val_set_regr_predictions))\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the linear regressor model on training data\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regr = RandomForestRegressor()\n",
    "regr.fit(ca_train2_set_regr, ca_train2_set_regr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411.34022978175216"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the performance on validation data\n",
    "\n",
    "ca_val_set_regr_predictions = regr.predict(ca_val_set_regr)\n",
    "error = np.sqrt(mean_squared_error(ca_val_set_regr_labels, ca_val_set_regr_predictions))\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient tree boosting for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the linear regressor model on training data\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "regr = GradientBoostingRegressor()\n",
    "regr.fit(ca_train2_set_regr, ca_train2_set_regr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398.0699115184063"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the performance on validation data\n",
    "\n",
    "ca_val_set_regr_predictions = regr.predict(ca_val_set_regr)\n",
    "error = np.sqrt(mean_squared_error(ca_val_set_regr_labels, ca_val_set_regr_predictions))\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression gave slighty better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://github.com/maalvarezl/MLAI-Labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Tandem Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prediction model based on two separate models in tandem \n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(ca_train2_set_clf_all_attributes, ca_train2_set_clf_labels)\n",
    "\n",
    "regr = Ridge()\n",
    "regr.fit(ca_train2_set_regr, ca_train2_set_regr_labels)\n",
    "\n",
    "pred_list = list()\n",
    "\n",
    "for data in ca_test_set_regr:\n",
    "    prediction = clf.predict(data.reshape(1,-1))\n",
    "    if prediction==1:\n",
    "        tandem_prediction = regr.predict(data.reshape(1,-1))\n",
    "    else:\n",
    "        tandem_prediction = prediction\n",
    "    pred_list.append(tandem_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1344"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Performance of tandem model over test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1143, 79)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_val_set_regr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_test_set_predictions = regr.predict(ca_test_set_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1344,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_test_set_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE on the validation data is : 433.61125762424405\n"
     ]
    }
   ],
   "source": [
    "## Computing the RMSE for the test dataset\n",
    "error_rr = np.sqrt(mean_squared_error(ca_test_set_regr_labels, ca_test_set_predictions))\n",
    "print('The RMSE on the validation data is :',error_rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RMSE of tandem model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462.9773798100883"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = np.sqrt(mean_squared_error(ca_test_set_regr_labels, pred_list))\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_train_set_attributes_prepared = full_transform.fit_transform(ca_train_set_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ridge regerssor performed the best in regression. Gradient Boost Classifier is the best in classifiers. But the RMSE of tandem model is higher than that of a single model. It is because of the cumulated error of both the classifier and the regressor.\n",
    "\n",
    "\n",
    "2. Out of the 30000 initial records 21000 had claim amount 0. Even after balancing 50% of them were non zero random numbers but 50% values are straight away 0. The error might be more due to models biasing more towards 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. My Insurance Claim Predictor Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def my_insurance_claim_predictor(Xtest=''):\n",
    "    train_df = pd.read_csv('./data/train.csv')\n",
    "    print(train_df.columns)\n",
    "    train_df = train_df.fillna('?')\n",
    "    train_df = train_df.drop(columns = ['Cat2', 'Cat4', 'Cat5', 'Cat7'])\n",
    "    train_df = train_df[train_df['Cat1'] != '?']\n",
    "    train_df = train_df[train_df['Cat3'] != '?']\n",
    "    train_df = train_df[train_df['Cat6'] != '?']\n",
    "    train_df = train_df[train_df['Cat8'] != '?']\n",
    "    train_df = train_df[train_df['Cat9'] != '?']\n",
    "    train_df = train_df[train_df['Cat10'] != '?']\n",
    "    train_df = train_df[train_df['Cat11'] != '?']\n",
    "    train_df = train_df[train_df['Cat12'] != '?']\n",
    "    train_df = train_df[train_df['Blind_Make'] != '?']\n",
    "    train_df = train_df[train_df['Blind_Model'] != '?']\n",
    "    train_df = train_df[train_df['Blind_Submodel'] != '?']\n",
    "    train_df = train_df[train_df['OrdCat'] != '?']\n",
    "    train_df = train_df.astype({'Household_ID': 'float64', 'Vehicle': 'float64','Calendar_Year': 'float64',\n",
    "                                'Model_Year': 'float64', 'OrdCat': 'float64',})\n",
    "    df_zeroes = train_df[train_df['Claim_Amount'] == 0]\n",
    "    df_non_zeroes = train_df[train_df['Claim_Amount'] != 0]\n",
    "\n",
    "    nz = df_non_zeroes['Claim_Amount'].count()\n",
    "\n",
    "    z = df_zeroes['Claim_Amount'].count()\n",
    "\n",
    "    if nz != z:\n",
    "        if nz > z:\n",
    "            df_non_zeroes = df_non_zeroes.sample(n = z)\n",
    "        elif nz < z:\n",
    "            df_zeroes = df_zeroes.sample(n = nz)\n",
    "    else:\n",
    "        print('Data set is balanced')\n",
    "        \n",
    "    claimed_amount_data = pd.concat([df_non_zeroes,df_zeroes], ignore_index=True)\n",
    "    claimed_amount_data = claimed_amount_data.drop('Row_ID', axis=1)\n",
    "    ca_train_set, ca_test_set = train_test_split(claimed_amount_data, test_size=0.2, random_state=42)\n",
    "    ca_train_set_attributes = ca_train_set.drop('Claim_Amount', axis=1)\n",
    "    \n",
    "    ca_train_set_labels = ca_train_set['Claim_Amount']\n",
    "    \n",
    "    attributes_cat = ['Cat1', 'Cat3', 'Cat6', 'Cat8', 'Cat9', 'Cat10', 'Cat11', 'Cat12', 'OrdCat', 'NVCat']\n",
    "    attributes_num = ['Household_ID', 'Vehicle', 'Calendar_Year', 'Model_Year', 'Var1',\n",
    "                      'Var2', 'Var3', 'Var4', 'Var5', 'Var6', 'Var7', 'Var8',\n",
    "                      'NVVar1', 'NVVar2', 'NVVar3', 'NVVar4']\n",
    "    full_transform = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), attributes_num),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown = \"ignore\"), attributes_cat),\n",
    "    ])\n",
    "\n",
    "    ca_train_set_attributes_prepared = full_transform.fit_transform(ca_train_set_attributes)\n",
    "    rr = Ridge()\n",
    "    rr.fit(ca_train_set_attributes_prepared, ca_train_set_labels)\n",
    "        \n",
    "    df = pd.read_csv(Xtest)\n",
    "    print(df.columns)\n",
    "    df = df.fillna('?')\n",
    "    if 'Unnamed: 34' in df.columns:\n",
    "        df = df.drop('Unnamed: 34', axis=1)\n",
    "    df = df.drop(columns = ['Row_ID', 'Cat2', 'Cat4', 'Cat5', 'Cat7'])\n",
    "    df = df[df['Cat1'] != '?']\n",
    "    df = df[df['Cat3'] != '?']\n",
    "    df = df[df['Cat6'] != '?']\n",
    "    df = df[df['Cat8'] != '?']\n",
    "    df = df[df['Cat9'] != '?']\n",
    "    df = df[df['Cat10'] != '?']\n",
    "    df = df[df['Cat11'] != '?']\n",
    "    df = df[df['Cat12'] != '?']\n",
    "    df = df[df['Blind_Make'] != '?']\n",
    "    df = df[df['Blind_Model'] != '?']\n",
    "    df = df[df['Blind_Submodel'] != '?']\n",
    "    df = df[df['OrdCat'] != '?']\n",
    "    df = df.astype({'Household_ID': 'float64', 'Vehicle': 'float64','Calendar_Year': 'float64', \n",
    "                    'Model_Year': 'float64', 'OrdCat': 'float64',})\n",
    "    \n",
    "    test_set_attributes = full_transform.transform(df)\n",
    "    predictions = rr.predict(test_set_attributes)\n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Row_ID', 'Household_ID', 'Vehicle', 'Calendar_Year', 'Model_Year',\n",
      "       'Blind_Make', 'Blind_Model', 'Blind_Submodel', 'Cat1', 'Cat2', 'Cat3',\n",
      "       'Cat4', 'Cat5', 'Cat6', 'Cat7', 'Cat8', 'Cat9', 'Cat10', 'Cat11',\n",
      "       'Cat12', 'OrdCat', 'Var1', 'Var2', 'Var3', 'Var4', 'Var5', 'Var6',\n",
      "       'Var7', 'Var8', 'NVCat', 'NVVar1', 'NVVar2', 'NVVar3', 'NVVar4',\n",
      "       'Claim_Amount'],\n",
      "      dtype='object')\n",
      "Index(['Row_ID', 'Household_ID', 'Vehicle', 'Calendar_Year', 'Model_Year',\n",
      "       'Blind_Make', 'Blind_Model', 'Blind_Submodel', 'Cat1', 'Cat2', 'Cat3',\n",
      "       'Cat4', 'Cat5', 'Cat6', 'Cat7', 'Cat8', 'Cat9', 'Cat10', 'Cat11',\n",
      "       'Cat12', 'OrdCat', 'Var1', 'Var2', 'Var3', 'Var4', 'Var5', 'Var6',\n",
      "       'Var7', 'Var8', 'NVCat', 'NVVar1', 'NVVar2', 'NVVar3', 'NVVar4',\n",
      "       'Unnamed: 34'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ACP20GPK\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n"
     ]
    }
   ],
   "source": [
    "pred_test = my_insurance_claim_predictor(Xtest='./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
